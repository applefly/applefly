<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[逻辑回归与线性回归原理探微]]></title>
      <url>http://onesimple.cn//blog/2017/02/logistic-regression/</url>
      <content type="html"><![CDATA[<p>&emsp;&emsp;回归和分类是机器学习解决的最重要的问题，本篇主要整理了线性回归和逻辑回归的基本推理过程和工程实践应用，首先对两种回归的方法做一下比较:</p>
<blockquote>
<p>线性回归: 最小二乘法推导，预测连续值</p>
<p>逻辑回归: 基于最大似然估计，主要解决二分类问题，也可用于连续值的预测</p>
</blockquote>
<a id="more"></a>
<h1 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h1><h2 id="问题引出"><a href="#问题引出" class="headerlink" title="问题引出"></a>问题引出</h2><p>&emsp;&emsp;房价y和房屋面积x之间的关系，我们一般估计为线性关系，即目标函数为:</p>
<p>$$<br>    f(x) = ax + b<br>$$<br>$$<br>   f(x) \approx  y<br>$$<br>根据样本很容易计算出a、b，也就能根据一个给定的面积x估算出房价了。\<br>如果有影响房价的不止一个因素(现实中往往如此，比如国民平均收入水平也是影响房价的一个因素)，就是一个多变量问题，这时我们估计的目标函数就变成多元的了，如果有m个因素,记为列向量:<br>$$\sum_{i=1}^n a_i=0$$</p>
<p>$$<br>    x = (x_0,x_1,…,x_{m-1})<br>$$<br>则要求解的参数w的也应该是m个，也记为列向量<br>$$<br>    w =<br>        \left (<br>            \begin{matrix}<br>                w_0 \\<br>                w_1 \\<br>                … \\<br>                w_{m-1}<br>            \end{matrix}<br>        \right )<br>$$<br>预测的f(x)是一个值，<code>f(x) = ax +b</code>的形式就变成:<br>$$<br>    f = xw<br>$$<br>推广到样本空间，记第i个样本为:<br>$$<br>    x_i = (x_{i0}, x_{i1},…,x_{i,m-1})<br>$$<br>如果有n个样本，样本空间表示为矩阵X:<br>$$<br>    X_{n \times m} =<br>        \left (<br>            \begin{matrix}<br>                x_{00} &amp; x_{01} &amp; … &amp; x_{0,m-1} \\<br>                x_{10} &amp; x_{11} &amp; … &amp; x_{1,m-1} \\<br>                …    &amp; … &amp; … &amp; …          \\<br>                x_{n-1,0} &amp; x_{n-1,1} &amp; … &amp; x_{n-1,m-1}<br>            \end{matrix}<br>        \right )<br>$$<br>则预测的所有f值的向量为:<br>$$<br>    f_{n \times 1} =<br>            \left (<br>            \begin{matrix}<br>                f_0 \\<br>                f_1 \\<br>                … \\<br>                f_{n-1}<br>            \end{matrix}<br>        \right )<br>        =<br>        X_{n \times m}w_{m \times 1}<br>$$<br>预测值和实际值一般会有一个误差，记为e<br>$$<br>  y_{n \times 1} = f_{n \times 1} + e_{n \times 1}<br>$$<br>任意第i个样本跟预测结果的误差的分布是随机的，既然是随机的，根据中心极限定理就可以估计其分布为正态分布，均值为0，方差为定值，则依据正态分布的概率密度定义,预测值与实际值之间误差的概率密度为:</p>
<p>$$<br>  p({e^{(i)}}) = \frac    {1}<br>      {<br>           \sqrt[2]  {2\pi}<br>           \sigma<br>      }<br>      e^{( - \frac { ( {e^{(i)}} ) ^ 2}<br>          {2\sigma^2}<br>      )}<br>$$<br>其中<br>$$<br>{e^{(i)}} = {y^{(i)}} - w_{m \times 1}^T {x_{m \times 1}^{(i)}}<br>$$<br>此式代入上式。假定n个样本独立，则n个样本的最大似然函数是各自概率密度的乘积:<br>$$<br>L(w) = \prod_{i=1}^{n} \frac    {1}<br>      {<br>           \sqrt[2]  {2\pi}<br>           \sigma<br>      }<br>      e^{( - \frac { ( {e^{(i)}} ) ^ 2}<br>          {2\sigma^2}<br>      )}<br>$$<br>此式是关于w的似然函数，要求的是w，可以先对上式取对数,得:<br>$$<br>l(w) = \sum_{i=1}^{n} \ln  [ {<br>       \frac    {1}<br>      {<br>           \sqrt[2]  {2\pi}<br>           \sigma<br>      }<br>      e^{( - \frac { ( {e^{(i)}} ) ^ 2}<br>          {2\sigma^2}<br>      )}<br>      } ]    \\\\<br>     = n \ln (\frac {1} { \sqrt[2]  {2\pi} \sigma }) -<br>     \frac {1} {2\sigma^2} \sum_{i=1}^{n} ({y^{(i)}} - w_{n \times 1}^T {x_{n \times 1}^{(i)}})^2<br>$$<br>前半项是常数项，记后半项为:<br>$$ \begin{equation}<br>J(w) = \frac {1} {2} {\sum_{i=1}^{n} ({y^{(i)}} - w_{n \times 1}^T {x_{n \times 1}^{(i)}})^2} \\\\<br>     = \frac {1} {2} {\sum_{i=1}^{n} (f_w(x) - y)^2}<br>     \label{a0} \end{equation}<br>$$</p>
<blockquote>
<p>l(w) J(w) 都是一个数值，根据最大似然估计法，概率总和最大时参数估计的最合理，则最大似然函数L(w)取最大时，可推出J(w)取最小，J(w)为最小二乘法的目标函数，问题变为求目标函数最小值的问题。</p>
</blockquote>
<h2 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h2><p>X为nxm的样本矩阵，Y为nx1维列向量，J(w)可以写成以下形式:<br>$$<br>\begin{equation}<br>J(w) = \frac {1} {2} (Xw - Y)^T(Xw - Y)<br>\label{a1}<br>\end{equation}<br>$$<br>要求J(w)的极值，可对上式求偏导:<br>$$<br>\nabla _w J(w) = \nabla _w (Xw - Y)^T(Xw - Y) \\\\<br>= \nabla _w (w^T X^T X w - w^T X^T Y - Y^T X w - Y^T Y) \\\\<br>= 2 X^T X w - X^T Y - (Y^T X)^T \\\\<br>= 2 ( X^T X w - X^T Y )<br>$$</p>
<p>注意:</p>
<blockquote>
<p>$Y^T X$  是常数项对w求偏导为0<br>$ \frac{\partial Xw}{\partial w} = X^\mathrm {T} $</p>
</blockquote>
<p>令上式偏导等于0，求解w:<br>$$<br>w = ( X^\mathrm {T} X ) ^ {-1} X^\mathrm {T} Y<br>$$<br>为防止$ X^\mathrm {T} X $ 不可逆或者防止过拟合，增加扰动$\lambda $<br>$$<br>\begin{equation}<br>w = ( X^\mathrm {T} X + \lambda I ) ^ {-1} X^\mathrm {T} Y<br>\label{a2}<br>\end{equation}<br>$$<br>I为单位阵，以上就是线性回归数理上的最终结论。</p>
<h2 id="正则化处理"><a href="#正则化处理" class="headerlink" title="正则化处理"></a>正则化处理</h2><p>过拟合是指在样本集上表现良好，但测试集上不好，一般是w中有些值过大，振荡很大，而导致过拟合。<br>为解决这个问题可以让更多的$w_j$取0，且越多越好，这种处理方法叫$L_0$正则化；<br>一般是让$ \sum_{j=1}^{m} |w_j| $ 取最小，这种处理叫$L_0$正则化；<br>如果是让$ \sum_{j=1}^{m} |w_j|^2 $ 取最小，则称谓$L_2$正则化。<br>故目标函数 $(\ref{a1})$ 加入平方和损失:<br>$$<br>\begin{equation}<br>J(w) = \frac {1} {2} (Xw - Y)^T(Xw - Y) + \lambda \sum_{j=1}^{m} {w_j}^2<br>\end{equation}<br>$$<br>根据此式进行求解也能推出w的求解公式$(\ref{a2})$</p>
<h2 id="训练与评价"><a href="#训练与评价" class="headerlink" title="训练与评价"></a>训练与评价</h2><p>怎么对样本进行训练呢?对于正则化项$\lambda$,怎么取值合理呢？</p>
<p>|  <strong><code>------- 1 -------</code></strong>    | <strong><code>-- 2 --</code></strong> |<strong><code>-- 3 --</code></strong>|</p>
<p>按合适比例划分样本，得到1、2、3三个部分<br>区间1用于训练w<br>区间2可以试用不同的$\lambda$，使用$w+\lambda I $作预测，看$\lambda$取多少时预测的准确，则选定此$\lambda$，这个过程不需要严格。<br>区间3对w、$\lambda$ 进行验证，当然，还有些交叉验证的策略。<br>|  <strong><code>------------  训练 -------------</code></strong>    | <strong><code>10% 验证</code></strong> |<br>|  <strong><code>--------  训练 ---------</code></strong>    | <strong><code>10%验证</code></strong> | <strong><code>- 训练-</code></strong> |<br>… …       交叉训练验证走十次       … …</p>
<h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>公式$(\ref{a2})$是参数w的解析解,但实际工程中样本空间很大，不可能对样本空间进行矩阵运算，所以这就需要机器学习的思路，从参数迭代的角度来求解w,梯度下降是常用的算法。</p>
<blockquote>
<p>其意义是，为了求得函数的极值点(局部最小值)，先任取一点，然后沿此点处梯度的负方向进行迭代到一个新的点，继续下降直到收敛，算法结束。</p>
</blockquote>
<p>首先我们来对$(\ref{a0})$某个$w_j$求偏导<br>$$<br> \frac{\partial J(w)}{\partial w_j} = \frac{\partial }{\partial w_j} { \frac {1} {2} (f_w (x) - y)^2 } \\\\<br> = (f_w (x) - y) \cdot \frac{\partial }{\partial w_j} (f_w (x) - y)\\\\<br> = (f_w (x) - y) \cdot \frac{\partial }{\partial w_j} (\sum_{k=1}^{m} w_k x_k - y) \\\\<br> = (f_w (x) - y) x_j<br>$$<br>注意，上式除了$w_j$其他都算常量，所以求导得到上式结果。结论可简记为：</p>
<blockquote>
<p>某个维度的梯度 = 预测误差 X 样本在该维度的取值</p>
</blockquote>
<p>既然有了这个梯度，我们就可以有以下几种方式计算。</p>
<h3 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h3><p><strong>批量梯度下降是对所有样本的梯度做加和，然后按步长进行下降迭代，直到收敛</strong></p>
<p>while  重复直到收敛 :  {<br>$$ w_j = w_j + \alpha \sum_{i=1}^{n} [ (y^{(i)} - f_w(x^{(i)})) \cdot x_j^{(i)} ] $$<br>}<br>$\alpha$为每次迭代的步长<br>凸函数局部最小值一定是全局最小值，批量梯度下降一定会收敛。<br>初始可以让w都为0</p>
<h3 id="随机梯度下降SGD"><a href="#随机梯度下降SGD" class="headerlink" title="随机梯度下降SGD"></a>随机梯度下降SGD</h3><p>随机梯度下降也称SGD<br><strong>针对每来一个样本进行一次迭代，而不是对所有样本的梯度做加和做迭代</strong><br>while 迭代次数  :  {<br>&emsp;&emsp;for i = 0 to n : {<br>$$<br>w_j = w_j + \alpha (y^{(i)} - f_w(x^{(i)})) \cdot x_j^{(i)}<br>$$<br>&emsp;&emsp;}<br>}<br>注:可以通过最大迭代次数来退出loop，也可以判断本次参数$w_j^{(k)}$ 和下次的$w_j^{(k+1)}$之间的误差，如果稳定到很小的值，则认为收敛，可以退出。</p>
<blockquote>
<p>全局梯度下降有可能在一些拐点位置停止不前，而随机梯度下降就或许能跳过一些拐点到达真正的收敛点附近，这是梯度下降的一些特点，以后有机会附文研讨。<br>随机梯度下降一般速度会比较快，学习步长也可以让它先大后小。</p>
</blockquote>
<h3 id="mini-batch梯度下降"><a href="#mini-batch梯度下降" class="headerlink" title="mini-batch梯度下降"></a>mini-batch梯度下降</h3><p>基于以上两种算法的折中算法，每次随机选取b个样本做一个批量梯度下降，这样既节省了计算整个批量的时间，同时计算的方向也会像SGD一样更加准确。<br>repeat until convergency{<br>&emsp;&emsp;for i=1;i&lt;n ; i+=b: {<br>$$ w_j = w_j + \alpha \sum_{i=1}^{i+b} [ (y^{(i)} - f_w(x^{(i)})) \cdot x_j^{(i)} ] $$<br>&emsp;&emsp;}<br>}
　</p>
<h1 id="2-非线性回归"><a href="#2-非线性回归" class="headerlink" title="2.非线性回归"></a>2.非线性回归</h1><h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p>其实只是特征增加了$x^2$项，y相对于特征是非线性的，相对于w还是线性的。往往可以先通过变量置换，$x^’ = x^2$把非线性回归化为线性回归，再利用线性回归的方法确定参数及估计值。</p>
<blockquote>
<p><strong>局部加权回归暂不介绍</strong></p>
</blockquote>
<h1 id="3-逻辑回归"><a href="#3-逻辑回归" class="headerlink" title="3.逻辑回归"></a>3.逻辑回归</h1><h2 id="问题引出-1"><a href="#问题引出-1" class="headerlink" title="问题引出"></a>问题引出</h2><p>线性回归适合预测连续值，在解决分类问题时则有些不合理，比如</p>
<center><br><img title="线性回归拟合直线" alt="线性回归拟合直线" class="class1 class2" src="http://okqpu7sb4.bkt.clouddn.com//image/data/line-rg.png?imageView2/2/w/600"><br></center>






]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[hexo博客之常用markdown语法及latex公式备忘]]></title>
      <url>http://onesimple.cn//blog/2017/01/write-blog-with-hexo/</url>
      <content type="html"><![CDATA[<p>本篇主要整理了一些常用的markdown语法，以及hexo博客使用mathjax渲染公式的一些样例。</p>
<a id="more"></a>
<ul>
<li>图片居中<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;center&gt;</div><div class="line">&#123;% qnimg user/alipay.jpg title:微信二维码 alt:二维码 &apos;class:class1 class2&apos; extend:?imageView2/2/w/600 %&#125;</div><div class="line">&lt;/center&gt;</div></pre></td></tr></table></figure>
</li>
</ul>
<center><br><img title="支付宝二维码" alt="二维码" class="class1 class2" src="http://okqpu7sb4.bkt.clouddn.com//image/user/alipay.jpg?imageView2/2/w/600"><br></center>

<ul>
<li>mathjax 公式测试<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">段内公式只用一个$即可:  $F_&#123;\mu&#125;$</div></pre></td></tr></table></figure>
</li>
</ul>
<p>&emsp;&emsp;这是我的段内公式: $F_{\mu}$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$$\sum_&#123;i=1&#125;^n a_i=0$$</div></pre></td></tr></table></figure>
<p>$$\sum_{i=1}^n a_i=0$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$F_&#123;\mu&#125;$</div><div class="line"></div><div class="line">$F_1 + F_b = F_c$</div><div class="line"></div><div class="line">$$</div><div class="line">f(x) \approx  y</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$F_{\mu}$</p>
<p>$F_1 + F_b = F_c$</p>
<p>$$<br>f(x) \approx  y<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$x = ( x_0 , x_1 , ... , x_&#123;m-1&#125; )$</div></pre></td></tr></table></figure>
<p>$x = ( x_0 , x_1 , … , x_{m-1} )$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$$\sum_&#123;i=1&#125;^n a_i=0$$</div></pre></td></tr></table></figure>
<p>$$\sum_{i=1}^n a_i=0$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$x = ( x_0 , x_1 , ... , x_&#123;m-1&#125; )$</div></pre></td></tr></table></figure>
<p>$x = ( x_0 , x_1 , … , x_{m-1} )$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\begin&#123;bmatrix&#125;</div><div class="line">1 &amp; 2\\</div><div class="line">3 &amp; 4</div><div class="line">\end&#123;bmatrix&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>\begin{bmatrix}<br>1 &amp; 2\\<br>3 &amp; 4<br>\end{bmatrix}<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">    w = </div><div class="line">        \left (</div><div class="line">            \begin&#123;matrix&#125; </div><div class="line">                w\_0 \\</div><div class="line">                w\_1 \\</div><div class="line">                ... \\</div><div class="line">                w\_&#123;m-1&#125; </div><div class="line">            \end&#123;matrix&#125;</div><div class="line">        \right )</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>    w =<br>        \left (<br>            \begin{matrix}<br>                w_0 \\<br>                w_1 \\<br>                … \\<br>                w_{m-1}<br>            \end{matrix}<br>        \right )<br>$$</p>
<ul>
<li>表格测试<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">| 链接 | 结果 | 原因 |</div><div class="line">|:-----|:---:|----------:|</div><div class="line">|文本内容| **`是`** |同协议同域名同端口|</div><div class="line">|文本内容| **`是`** |同协议同域名同端口|</div><div class="line">|文本内容| **`是`** |同协议同域名同端口|</div></pre></td></tr></table></figure>
</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:left">链接</th>
<th style="text-align:center">结果</th>
<th style="text-align:right">原因</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">文本内容</td>
<td style="text-align:center"><strong><code>是</code></strong></td>
<td style="text-align:right">同协议同域名同端口</td>
</tr>
<tr>
<td style="text-align:left">文本内容</td>
<td style="text-align:center"><strong><code>是</code></strong></td>
<td style="text-align:right">同协议同域名同端口</td>
</tr>
<tr>
<td style="text-align:left">文本内容</td>
<td style="text-align:center"><strong><code>是</code></strong></td>
<td style="text-align:right">同协议同域名同端口</td>
</tr>
</tbody>
</table>
]]></content>
    </entry>
    
  
  
</search>
